To improve the program's resilience (specifically so a failed Summary doesn't block the other fields) and the quality of the responses, we need to update generated_backend/worker.py.

Here is the Robust Fix implementation. It refactors the worker to split the "Summary" task (which is prone to errors) from the "Data Extraction" task (which is usually stable).

Updated generated_backend/worker.py
Replace your entire worker.py with this version. I have added comments explaining the Improvements in the process_po_job function.

Python

import os
import requests
import json
import time
import logging
from typing import Dict, Any, Union
from redis import Redis
from rq import Worker, Queue, Connection

# Configure Logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
OLLAMA_URL = os.getenv('OLLAMA_URL', 'http://localhost:11434/api/generate')
QUICKBASE_URL = os.getenv('QUICKBASE_URL', 'https://api.quickbase.com/v1/records')
QUICKBASE_USER_TOKEN = os.getenv('QUICKBASE_USER_TOKEN')
QUICKBASE_REALM = os.getenv('QUICKBASE_REALM', 'your-realm.quickbase.com')

# Redis Connection (Global)
redis_conn = Redis(
    host=os.getenv('REDIS_HOST', 'localhost'),
    port=int(os.getenv('REDIS_PORT', 6379)),
    password=os.getenv('REDIS_PASSWORD', None)
)

def log_safe_event(message: str):
    """Logs a message to stdout (for systemd) AND pushes it to Redis (for Dashboard)."""
    logger.info(message)
    timestamp = time.strftime("%H:%M:%S")
    log_entry = f"[{timestamp}] {message}"
    try:
        # Push to 'monitor_logs' list and keep only last 100
        redis_conn.lpush("monitor_logs", log_entry)
        redis_conn.ltrim("monitor_logs", 0, 99) 
    except Exception as e:
        logger.warning(f"Could not push log to Redis: {e}")

def query_ollama(po_text: str, prompt_json: Dict[str, str], is_summary: bool = False) -> Dict[str, Any]:
    """
    Send text + schema to Ollama.
    IMPROVEMENT: Added 'is_summary' flag to adjust settings dynamically.
    """
    if is_summary:
        system_instruction = (
            "You are an expert legal analyst. Summarize the document sections as requested. "
            "Use <br> tags for line breaks. Do not use newlines inside values. "
            "Output ONLY valid JSON."
        )
        # Summaries need more "creativity" and length
        options = {
            "temperature": 0.1, 
            "num_predict": 4096, 
            "top_k": 40, 
            "num_ctx": 32000,
            "repeat_penalty": 1.1
        }
    else:
        system_instruction = (
            "You are a strict data extraction engine. Extract specific terms exactly as they appear. "
            "Output ONLY valid JSON."
        )
        # Data extraction needs to be deterministic (0 temperature)
        options = {
            "temperature": 0.0, 
            "num_predict": 1024, 
            "top_k": 20, 
            "num_ctx": 32000
        }

    full_prompt = f"""
{system_instruction}

--- BEGIN DOCUMENT TEXT ---
{po_text}
--- END DOCUMENT TEXT ---

**INSTRUCTIONS:**
1. Read the document text above.
2. For each key in the schema below, provide the requested information.
3. Return ONLY the JSON object.

**REQUIRED OUTPUT SCHEMA:**
{json.dumps(prompt_json, indent=2)}
"""

    payload = {
        "model": "llama3.1",
        "format": "json",
        "prompt": full_prompt,
        "stream": False,
        "options": options
    }

    try:
        response = requests.post(OLLAMA_URL, json=payload, timeout=1800)
        response.raise_for_status()
        return json.loads(response.json().get('response', '{}'))
    except requests.RequestException as e:
        log_safe_event(f"Error querying Ollama: {e}")
        raise
    except json.JSONDecodeError as e:
        log_safe_event(f"Error parsing AI response: {e}")
        raise

def update_quickbase(record_id: str, target_table_id: str, target_field_ids: Dict[str, int], ai_data: Dict[str, Any]):
    """Update the record in Quickbase using the dynamic field map."""
    headers = {
        'QB-Realm-Hostname': QUICKBASE_REALM,
        'User-Agent': 'Python-Worker',
        'Authorization': f'QB-USER-TOKEN {QUICKBASE_USER_TOKEN}',
        'Content-Type': 'application/json'
    }

    fields_to_update = {}
    fields_to_update["3"] = {"value": record_id}

    for json_key, fid in target_field_ids.items():
        if json_key in ai_data:
            fields_to_update[str(fid)] = {"value": ai_data[json_key]}
        else:
            # IMPROVEMENT: Don't just warn, check if we have a partial result or error message
            logger.warning(f"Key '{json_key}' missing from AI response.")

    if len(fields_to_update) <= 1:
        logger.warning("No new data fields to update.")
        return {}

    body = {
        "to": target_table_id,
        "data": [fields_to_update]
    }

    try:
        response = requests.post(QUICKBASE_URL, headers=headers, json=body)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        log_safe_event(f"Error updating Quickbase: {e}")
        logger.error(f"Failed Payload Metadata: {{'to': target_table_id, 'record_id': record_id, 'error': str(e)}}")
        raise

def update_quickbase_error(record_id: str, target_table_id: str, error_field_id: int, error_message: str):
    headers = {
        'QB-Realm-Hostname': QUICKBASE_REALM,
        'User-Agent': 'Python-Worker',
        'Authorization': f'QB-USER-TOKEN {QUICKBASE_USER_TOKEN}',
        'Content-Type': 'application/json'
    }

    body = {
        "to": target_table_id,
        "data": [
            {
                "3": {"value": record_id},
                str(error_field_id): {"value": str(error_message)}
            }
        ]
    }

    try:
        requests.post(QUICKBASE_URL, headers=headers, json=body)
    except Exception as e:
        logger.error(f"Failed to report error to QuickBase: {e}")

def process_po_job(data: Dict[str, Any]):
    start_time = time.time()
    record_id = data['record_id']
    request_name = data.get('request_name', 'Unknown Request')
    
    log_safe_event(f"Processing Job: '{request_name}' for Record: {record_id}")

    # --- GUARD RAILS ---
    if not data.get('po_text') or len(data.get('po_text', '').strip()) < 10:
        log_safe_event("Skipped: Text empty.")
        return "Skipped"

    # --- IMPROVED SPLIT LOGIC ---
    full_prompt_map = data.get('prompt_json', {})
    
    # 1. Identify keys based on name "summary" or "description"
    summary_keys = [k for k in full_prompt_map.keys() if 'summary' in k.lower() or 'description' in k.lower()]
    extraction_keys = [k for k in full_prompt_map.keys() if k not in summary_keys]

    final_results = {}
    any_success = False

    # 2. Run Strict Extraction (Dates, Amounts) - HIGH IMPORTANCE
    if extraction_keys:
        try:
            logger.info(f"Extracting Data fields: {extraction_keys}")
            extraction_prompt = {k: full_prompt_map[k] for k in extraction_keys}
            
            # Call with is_summary=False for strict settings
            res = query_ollama(data['po_text'], extraction_prompt, is_summary=False)
            final_results.update(res)
            any_success = True
        except Exception as e:
            log_safe_event(f"Extraction failed: {e}")
            # We continue to try summarization even if extraction failed

    # 3. Run Summarization (Isolated) - LOWER IMPORTANCE / HIGHER RISK
    if summary_keys:
        try:
            logger.info(f"Generating Summary fields: {summary_keys}")
            summary_prompt = {k: full_prompt_map[k] for k in summary_keys}
            
            # Call with is_summary=True for creative settings
            res = query_ollama(data['po_text'], summary_prompt, is_summary=True)
            final_results.update(res)
            any_success = True
        except Exception as e:
            log_safe_event(f"Summarization failed: {e}")
            # Add error message to the summary field so user knows it failed
            for k in summary_keys:
                final_results[k] = f"Error: {str(e)[:100]}..."

    # 4. Update QuickBase with aggregated results
    try:
        if not any_success:
            raise Exception("Both Extraction and Summarization steps failed.")

        logger.info(f"Updating QuickBase table {data['target_table_id']}...")
        update_quickbase(
            record_id,
            data['target_table_id'],
            data['target_field_ids'],
            final_results
        )
        
        end_time = time.time()
        duration = end_time - start_time
        log_safe_event(f"Job complete for {record_id}")
        log_safe_event(f"PERFORMANCE: Job finished in {duration:.2f} seconds")
        return "Success"

    except Exception as e:
        log_safe_event(f"Job failed for {record_id}: {e}")
        if 'error_field_id' in data and 'target_table_id' in data:
            logger.info(f"Reporting error to QuickBase field {data['error_field_id']}...")
            update_quickbase_error(
                record_id,
                data['target_table_id'],
                data['error_field_id'],
                str(e)
            )
        raise e

if __name__ == '__main__':
    # Read queues from Env Var, default to 'default'
    queue_names = os.getenv('WORKER_QUEUES', 'default').split(',')
    
    with Connection(redis_conn):
        logger.info(f"Worker listening on queues: {queue_names}")
        worker = Worker(map(Queue, queue_names))
        worker.work()
Key Improvements in this Code:
Isolation of Failure: The code now splits your prompt into two batches: extraction_keys (like Date, Amount) and summary_keys. It runs them as separate AI calls. If the "Summary" call crashes (e.g., due to JSON formatting), the "Extraction" data is preserved and still sent to QuickBase.

Contextual Tuning: I modified query_ollama to accept an is_summary flag.

Data Extraction: Uses temperature: 0.0 (Strict, consistent).

Summaries: Uses temperature: 0.1 and a higher num_predict (More room to write text, slightly more creative).

Partial Failure Feedback: If the summary specifically fails, the code now writes "Error: [Reason]..." into the summary field in QuickBase instead of failing the whole job. This allows you to see why a specific document failed directly in your report.