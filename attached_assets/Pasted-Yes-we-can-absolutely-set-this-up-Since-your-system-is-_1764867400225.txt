Yes, we can absolutely set this up.

Since your system is "headless" and server-based, the most robust way to do this is to:

Update the Worker to explicitly log the "Time to Complete" (it wasn't calculating this before).

Add a Reporting Script that scans the daily logs, calculates stats, and sends the email.

Schedule it with a simple timer on the server.

Step 1: Update generated_backend/worker.py
We need to add timing logic so the logs explicitly state how long each job took.

Changes:

Import time.

Capture start_time and end_time.

Print the duration in a format our report script can easily read.

Replace your generated_backend/worker.py with this updated version:

Python

import os
import requests
import json
import time
from redis import Redis
from rq import Worker, Queue, Connection

# Configuration
OLLAMA_URL = os.getenv('OLLAMA_URL', 'http://localhost:11434/api/generate')
QUICKBASE_URL = os.getenv('QUICKBASE_URL', 'https://api.quickbase.com/v1/records')
QUICKBASE_USER_TOKEN = os.getenv('QUICKBASE_USER_TOKEN')
QUICKBASE_REALM = os.getenv('QUICKBASE_REALM', 'your-realm.quickbase.com')

def query_ollama(po_text, prompt_json):
    """
    Send text + schema to Ollama with HIGH FOCUS settings.
    """
    system_instruction = (
        "You are an expert legal contract analyst. "
        "Your job is to read the ENTIRE document provided and generate accurate, comprehensive summaries "
        "for the specific topics requested in the JSON schema. "
        "Do not skip sections. Synthesize information from multiple pages if necessary. "
        "If a topic is not present, explicitly state that it is not addressed. "
        "Output ONLY valid JSON."
    )

    full_prompt = f"""
{system_instruction}

--- BEGIN DOCUMENT TEXT ---
{po_text}
--- END DOCUMENT TEXT ---

**INSTRUCTIONS:**
1. Read the document text above.
2. For each key in the schema below, analyze the text and provide the requested summary.
3. Return ONLY the JSON object.

**REQUIRED OUTPUT SCHEMA:**
{json.dumps(prompt_json, indent=2)}
"""

    payload = {
        "model": "llama3.1",
        "format": "json",
        "prompt": full_prompt,
        "stream": False,
        "options": {
            "temperature": 0.0,
            "num_predict": 4096,
            "top_k": 20,
            "num_ctx": 32000,
            "repeat_penalty": 1.1
        }
    }

    try:
        response = requests.post(OLLAMA_URL, json=payload, timeout=1800)
        response.raise_for_status()
        return json.loads(response.json().get('response', '{}'))
    except requests.RequestException as e:
        print(f"Error querying Ollama: {e}")
        raise
    except json.JSONDecodeError as e:
        print(f"Error parsing AI response: {e}")
        raise

def update_quickbase(record_id, target_table_id, target_field_ids, ai_data):
    """
    Update the record in Quickbase using the dynamic field map.
    """
    headers = {
        'QB-Realm-Hostname': QUICKBASE_REALM,
        'User-Agent': 'Python-Worker',
        'Authorization': f'QB-USER-TOKEN {QUICKBASE_USER_TOKEN}',
        'Content-Type': 'application/json'
    }

    fields_to_update = {}
    fields_to_update["3"] = {"value": record_id}

    for json_key, fid in target_field_ids.items():
        if json_key in ai_data:
            fields_to_update[str(fid)] = {"value": ai_data[json_key]}
        else:
            print(f"WARNING: Expected key '{json_key}' not found in AI response.")

    body = {
        "to": target_table_id,
        "data": [fields_to_update]
    }

    try:
        response = requests.post(QUICKBASE_URL, headers=headers, json=body)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        print(f"Error updating Quickbase: {e}")
        print(f"Failed Payload Metadata: {{'to': target_table_id, 'record_id': record_id, 'error': str(e)}}")
        raise

def update_quickbase_error(record_id, target_table_id, error_field_id, error_message):
    headers = {
        'QB-Realm-Hostname': QUICKBASE_REALM,
        'User-Agent': 'Python-Worker',
        'Authorization': f'QB-USER-TOKEN {QUICKBASE_USER_TOKEN}',
        'Content-Type': 'application/json'
    }

    body = {
        "to": target_table_id,
        "data": [
            {
                "3": {"value": record_id},
                str(error_field_id): {"value": str(error_message)}
            }
        ]
    }

    try:
        requests.post(QUICKBASE_URL, headers=headers, json=body)
    except Exception as e:
        print(f"Failed to report error to QuickBase: {e}")

def process_po_job(data):
    start_time = time.time() # Start Timer
    record_id = data['record_id']
    request_name = data.get('request_name', 'Unknown Request')
    
    print(f"Processing Job: '{request_name}' for Record: {record_id}")

    # --- GUARD RAILS ---
    if not data.get('po_text') or len(data['po_text'].strip()) < 10:
        print("Skipped: Text empty.")
        return "Skipped"

    try:
        print(f"Sending to Ollama...")
        ai_result = query_ollama(data['po_text'], data['prompt_json'])
        print(f"Ollama analysis complete. Keys found: {list(ai_result.keys())}")

        print(f"Updating QuickBase table {data['target_table_id']}...")
        update_quickbase(
            record_id,
            data['target_table_id'],
            data['target_field_ids'],
            ai_result
        )
        
        end_time = time.time() # End Timer
        duration = end_time - start_time
        print(f"Job complete for {record_id}")
        print(f"PERFORMANCE: Job finished in {duration:.2f} seconds") # Log Duration
        return "Success"

    except Exception as e:
        print(f"Job failed for {record_id}: {e}")
        if 'error_field_id' in data and 'target_table_id' in data:
            print(f"Reporting error to QuickBase field {data['error_field_id']}...")
            update_quickbase_error(
                record_id,
                data['target_table_id'],
                data['error_field_id'],
                str(e)
            )
        raise e

if __name__ == '__main__':
    redis_conn = Redis(
        host=os.getenv('REDIS_HOST', 'localhost'),
        port=int(os.getenv('REDIS_PORT', 6379)),
        password=os.getenv('REDIS_PASSWORD', None)
    )

    with Connection(redis_conn):
        worker = Worker(map(Queue, ['default']))
        worker.work()
Step 2: Create the Reporting Script
Create a new file in Replit named generated_backend/daily_report.py. This script will read the server logs, calculate the stats, and email you.

Note: You will need an SMTP server (like Gmail, Outlook, or Azure Communication Services) to send the email.

Python

import smtplib
import subprocess
import os
import re
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from datetime import datetime

# --- CONFIGURATION ---
SMTP_SERVER = os.getenv("SMTP_SERVER", "smtp.office365.com")
SMTP_PORT = int(os.getenv("SMTP_PORT", 587))
SMTP_USER = os.getenv("SMTP_USER", "your_email@ampowersys.com")
SMTP_PASSWORD = os.getenv("SMTP_PASSWORD", "your_password")
EMAIL_TO = "bengillihan@ampowersys.com"
# ---------------------

def get_logs():
    # Fetch logs for the AI Worker from the last 24 hours (since midnight)
    try:
        # Note: 'today' in journalctl means 00:00:00 of the current day
        cmd = ["journalctl", "-u", "ai-worker", "--since", "today", "--no-pager"]
        result = subprocess.check_output(cmd, text=True)
        return result.splitlines()
    except subprocess.CalledProcessError as e:
        print(f"Error fetching logs: {e}")
        return []

def analyze_logs(log_lines):
    stats = {
        "total_jobs": 0,
        "success_jobs": 0,
        "failed_jobs": 0,
        "total_duration": 0.0,
        "errors": []
    }

    for line in log_lines:
        if "Processing Job:" in line:
            stats["total_jobs"] += 1
        
        if "Job complete for" in line:
            stats["success_jobs"] += 1
            
        if "Job failed for" in line:
            stats["failed_jobs"] += 1
            # Extract error message (everything after 'Job failed for ...:')
            parts = line.split(":", 3)
            if len(parts) > 3:
                stats["errors"].append(parts[3].strip())
            else:
                stats["errors"].append(line)

        # Look for the PERFORMANCE tag we added to worker.py
        # Log format: ... PERFORMANCE: Job finished in 123.45 seconds
        if "PERFORMANCE: Job finished in" in line:
            match = re.search(r"finished in (\d+\.\d+) seconds", line)
            if match:
                stats["total_duration"] += float(match.group(1))

    return stats

def send_email(stats):
    avg_time = 0
    if stats["success_jobs"] > 0:
        avg_time = stats["total_duration"] / stats["success_jobs"]

    subject = f"AI Worker Daily Report - {datetime.now().strftime('%Y-%m-%d')}"
    
    body = f"""
    <h2>Daily Processing Summary</h2>
    <p><b>Date:</b> {datetime.now().strftime('%Y-%m-%d')}</p>
    
    <h3>Performance Metrics</h3>
    <ul>
        <li><b>Total Jobs Attempted:</b> {stats['total_jobs']}</li>
        <li><b>Successful Jobs:</b> {stats['success_jobs']}</li>
        <li><b>Failed Jobs:</b> {stats['failed_jobs']}</li>
        <li><b>Average Processing Time:</b> {avg_time:.2f} seconds</li>
    </ul>

    <h3>Issues & Errors</h3>
    { "<ul>" + "".join([f"<li>{e}</li>" for e in stats['errors']]) + "</ul>" if stats['errors'] else "<p>No errors recorded today.</p>" }
    
    <hr>
    <p><i>Sent from Azure AI Processor</i></p>
    """

    msg = MIMEMultipart()
    msg['From'] = SMTP_USER
    msg['To'] = EMAIL_TO
    msg['Subject'] = subject
    msg.attach(MIMEText(body, 'html'))

    try:
        server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)
        server.starttls()
        server.login(SMTP_USER, SMTP_PASSWORD)
        server.send_message(msg)
        server.quit()
        print("Report email sent successfully.")
    except Exception as e:
        print(f"Failed to send email: {e}")

if __name__ == "__main__":
    logs = get_logs()
    if logs:
        statistics = analyze_logs(logs)
        send_email(statistics)
    else:
        print("No logs found for today.")