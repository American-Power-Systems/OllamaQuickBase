Yes, you can absolutely achieve both goals using Redis Queue (RQ). To do this, we need to make your "Router" (app.py) smarter so it decides which queue to send a job to, and then configure your two workers to listen to specific queues.

Here is the strategy:

The "Long Doc" Worker: Listens to a long_docs queue.

The "Fast" Worker: Listens to high, default, and low queues (in that order of priority).

The Router (app.py): Checks the text length first. If it's huge, it goes to long_docs. If not, it checks the Table ID to assign priority.

Step 1: Update generated_backend/app.py
We need to instantiate multiple queues and add the logic to route traffic.

Python

from flask import Flask, request, jsonify
from redis import Redis
from rq import Queue
import os

app = Flask(__name__)

# SECURITY: Your API Key
API_KEY = os.getenv('API_KEY', 'q#*3VyUK6&ih63xZ')

# Redis connection
redis_conn = Redis(
    host=os.getenv('REDIS_HOST', 'localhost'),
    port=int(os.getenv('REDIS_PORT', 6379)),
    password=os.getenv('REDIS_PASSWORD', None)
)

# Define our Queues
q_high = Queue('high', connection=redis_conn)
q_default = Queue('default', connection=redis_conn)
q_low = Queue('low', connection=redis_conn)
q_long = Queue('long_docs', connection=redis_conn)

# Configuration
LONG_DOC_THRESHOLD = 20000  # Characters (approx 5k tokens)

@app.route('/api/process_po', methods=['POST'])
def process_po():
    client_key = request.headers.get('X-API-Key')
    if client_key != API_KEY:
        return jsonify({'error': 'Unauthorized'}), 401

    try:
        data = request.get_json()
        
        # Validation
        required_fields = ['record_id', 'po_text', 'target_table_id', 'target_field_ids', 'prompt_json']
        if any(f not in data for f in required_fields):
            return jsonify({'error': 'Missing fields'}), 400

        # --- ROUTING LOGIC ---
        table_id = data.get('target_table_id', '')
        text_len = len(data.get('po_text', ''))
        
        selected_queue = q_default # Fallback
        queue_name = "default"

        # 1. Check Length First (Route to dedicated heavy worker)
        if text_len > LONG_DOC_THRESHOLD:
            selected_queue = q_long
            queue_name = "long_docs"
        
        # 2. Check Priority (If not a long doc)
        else:
            if table_id == 'brbf729zp':
                selected_queue = q_high
                queue_name = "high"
            elif table_id == 'bpcdfksyx':
                selected_queue = q_low
                queue_name = "low"
            else:
                selected_queue = q_default
                queue_name = "default"

        # Enqueue
        from worker import process_po_job
        job = selected_queue.enqueue(
            process_po_job,
            args=(data,),
            job_timeout='30m'
        )
        
        return jsonify({
            'status': 'queued',
            'queue': queue_name,
            'job_id': job.get_id(),
            'message': f'Record {data["record_id"]} added to {queue_name} queue'
        }), 202
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
Step 2: Update generated_backend/worker.py
We need to update the worker so it doesn't just listen to ['default'] anymore. Instead, it should read an environment variable to know which queues it is responsible for.

Change the bottom if __name__ == '__main__': block to this:

Python

if __name__ == '__main__':
    redis_conn = Redis(
        host=os.getenv('REDIS_HOST', 'localhost'),
        port=int(os.getenv('REDIS_PORT', 6379)),
        password=os.getenv('REDIS_PASSWORD', None)
    )

    # Read queues from Env Var (comma separated), default to 'default'
    # Example: "high,default,low" or "long_docs"
    queue_names = os.getenv('WORKER_QUEUES', 'default').split(',')
    
    with Connection(redis_conn):
        print(f"Worker listening on queues: {queue_names}")
        worker = Worker(map(Queue, queue_names))
        worker.work()